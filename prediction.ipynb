{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f974043",
   "metadata": {},
   "source": [
    "# Prediction (Supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fcdab",
   "metadata": {},
   "source": [
    "### Dataset and Problem Setup\n",
    "\n",
    "This notebook focuses on the supervised learning part of the project.  \n",
    "The goal is to predict whether an e-commerce session results in a purchase, using behavioural features from the Online Shoppers Purchasing Intention dataset. The target variable is `Revenue`, which indicates whether a purchase was completed during a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a22512a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12330 entries, 0 to 12329\n",
      "Data columns (total 18 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Administrative           12330 non-null  int64  \n",
      " 1   Administrative_Duration  12330 non-null  float64\n",
      " 2   Informational            12330 non-null  int64  \n",
      " 3   Informational_Duration   12330 non-null  float64\n",
      " 4   ProductRelated           12330 non-null  int64  \n",
      " 5   ProductRelated_Duration  12330 non-null  float64\n",
      " 6   BounceRates              12330 non-null  float64\n",
      " 7   ExitRates                12330 non-null  float64\n",
      " 8   PageValues               12330 non-null  float64\n",
      " 9   SpecialDay               12330 non-null  float64\n",
      " 10  Month                    12330 non-null  object \n",
      " 11  OperatingSystems         12330 non-null  int64  \n",
      " 12  Browser                  12330 non-null  int64  \n",
      " 13  Region                   12330 non-null  int64  \n",
      " 14  TrafficType              12330 non-null  int64  \n",
      " 15  VisitorType              12330 non-null  object \n",
      " 16  Weekend                  12330 non-null  bool   \n",
      " 17  Revenue                  12330 non-null  bool   \n",
      "dtypes: bool(2), float64(7), int64(7), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Revenue\n",
       "False    0.845255\n",
       "True     0.154745\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"online_shoppers_intention.csv\")\n",
    "\n",
    "df.head()\n",
    "df.info()\n",
    "df[\"Revenue\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4948f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Revenue\", axis=1)\n",
    "y = df[\"Revenue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f58db",
   "metadata": {},
   "source": [
    "### Train–Test Split\n",
    "\n",
    "The dataset is split into training and test sets before any preprocessing or model training.  \n",
    "Stratified sampling is used to preserve the imbalance between purchasing and non-purchasing sessions in both sets and to ensure a fair evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2338ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222ff85",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Numerical and categorical features are processed separately.  \n",
    "Numerical features are standardised, while categorical features are encoded using one-hot encoding. The preprocessing steps are later integrated into the model pipelines to avoid data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f157f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define categorical and numeric features explicitly\n",
    "categorical_features = [\n",
    "    \"Month\", \"VisitorType\", \"Weekend\",\n",
    "    \"Browser\", \"Region\", \"TrafficType\", \"OperatingSystems\"\n",
    "]\n",
    "\n",
    "numeric_features = [c for c in X_train.columns if c not in categorical_features]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581742a",
   "metadata": {},
   "source": [
    "### Supervised Learning Models\n",
    "\n",
    "Three supervised classification models are defined: Logistic Regression, Decision Tree, and Random Forest.  \n",
    "Logistic Regression is used as an interpretable baseline model, Decision Trees capture non-linear decision rules, and Random Forest is included as a robust ensemble model suitable for tabular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51eab17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# TODO (if not already done) - use Hyperparameter Tuning to find best parameters for each model - or just do it on the best one\n",
    "logreg_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "dt_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", DecisionTreeClassifier(\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680237df",
   "metadata": {},
   "source": [
    "> Using class_weight=balanced and such the model is forced to treat one error on a \"Buyer\" as equally bad as multiple errors on \"Non-Buyers.\" This effectively neutralizes the bias toward the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e2af8",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Random Forest)\n",
    "\n",
    "Hyperparameter tuning is performed for the Random Forest model to improve baseline performance while keeping the experimental setup controlled.  \n",
    "A limited cross-validated search is used to keep computational cost proportional to the project scope. The tuned model is later used for final evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b976734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best CV ROC-AUC: 0.9280743513199856\n",
      "Best params: {'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 5, 'model__n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Random Forest (baseline)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedGroupKFold\n",
    "\n",
    "# Force y to a plain 1D int array (0/1) to avoid any dtype edge-cases\n",
    "y_train_fixed = (\n",
    "    y_train.values.ravel() if hasattr(y_train, \"values\") else np.array(y_train).ravel()\n",
    ").astype(int)\n",
    "\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [200, 400],\n",
    "    \"model__max_depth\": [None, 20],\n",
    "    \"model__min_samples_leaf\": [1, 5],\n",
    "    \"model__max_features\": [\"sqrt\"],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train_fixed)\n",
    "\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "print(\"Best CV ROC-AUC:\", grid.best_score_)\n",
    "print(\"Best params:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b569f4a",
   "metadata": {},
   "source": [
    "### Baseline Model Evaluation\n",
    "\n",
    "The supervised models are evaluated using multiple metrics, including precision, recall, F1-score, ROC-AUC, and log-loss.  \n",
    "Using several metrics is necessary due to class imbalance and to assess both classification performance and probability quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07d10431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90      2084\n",
      "           1       0.49      0.74      0.59       382\n",
      "\n",
      "    accuracy                           0.84      2466\n",
      "   macro avg       0.72      0.80      0.75      2466\n",
      "weighted avg       0.88      0.84      0.85      2466\n",
      "\n",
      "ROC-AUC: 0.8932442142074746\n",
      "Log-loss: 0.45588421375820165\n",
      "\n",
      "=== Decision Tree ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91      2084\n",
      "           1       0.52      0.53      0.52       382\n",
      "\n",
      "    accuracy                           0.85      2466\n",
      "   macro avg       0.72      0.72      0.72      2466\n",
      "weighted avg       0.85      0.85      0.85      2466\n",
      "\n",
      "ROC-AUC: 0.7195322627649204\n",
      "Log-loss: 5.364160905841848\n",
      "\n",
      "=== Random Forest ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      2084\n",
      "           1       0.77      0.47      0.58       382\n",
      "\n",
      "    accuracy                           0.90      2466\n",
      "   macro avg       0.84      0.72      0.76      2466\n",
      "weighted avg       0.89      0.90      0.89      2466\n",
      "\n",
      "ROC-AUC: 0.9187859884836851\n",
      "Log-loss: 0.25754333726782075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss\n",
    "\n",
    "# TODO - maybe add Youden's Index to this so it is not just evaluated as an afterthought in the end\n",
    "def evaluate (model, X_train, y_train, X_test, y_test, name=\"model\"):\n",
    "    y_train_int = np.array(y_train).astype(int).ravel()\n",
    "    y_test_int = np.array(y_test).astype(int).ravel() \n",
    "    \n",
    "    model.fit(X_train, y_train_int)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_test_int, y_pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test_int,y_proba))\n",
    "    print(\"Log-loss:\", log_loss(y_test_int, y_proba))\n",
    "\n",
    "evaluate(logreg_pipeline, X_train, y_train, X_test, y_test, \"Logistic Regression\")\n",
    "evaluate(dt_pipeline, X_train, y_train, X_test, y_test, \"Decision Tree\")\n",
    "evaluate(rf_pipeline, X_train, y_train, X_test, y_test, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e1226e",
   "metadata": {},
   "source": [
    "### Threshold Optimisation (Logistic Regression)\n",
    "\n",
    "In addition to standard evaluation, Youden’s Index is used to analyse the effect of different classification thresholds for Logistic Regression.  \n",
    "This analysis illustrates how threshold choice influences the trade-off between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11f6a6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold (Youden): 0.4075656607707226\n",
      "Youden's Index: 0.6184492166695139\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logreg_pipeline.fit(X_train,y_train)\n",
    "proba = logreg_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, proba)\n",
    "youden = tpr - fpr \n",
    "best_idx = np.argmax(youden)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(\"Best threshold (Youden):\", best_threshold)\n",
    "print(\"Youden's Index:\", youden[best_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "purchase-intent-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
