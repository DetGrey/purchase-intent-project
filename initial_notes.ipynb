{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15804915",
   "metadata": {},
   "source": [
    "### Understanding the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a2918",
   "metadata": {},
   "source": [
    "There are two types of data:\n",
    "\n",
    "- Numerical: PageValues, ProductRelated_Duration, BounceRates\n",
    "- Categorical: Month, VisitorType, Region, OperatingSystems\n",
    "\n",
    "> This means that we must make use of **One-Hot enconding** or similar to make all data numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef7b0a",
   "metadata": {},
   "source": [
    "There is a long tail as most users spend little time on the site while others spend hours.\n",
    "\n",
    "> **Standardisation** would be needed to make sure the clusters won't be skewed to the side because of the few users that spend a long time on the site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9941cf",
   "metadata": {},
   "source": [
    "### Convex vs concave\n",
    "\n",
    "The Assumption (Convex): K-Means (which uses the Elbow method) assumes clusters are spherical blobs (like a ball or an orange). It tries to find the center of the ball. This is \"convex\" geometry.\n",
    "- The standard clustering algorithm.\n",
    "- It is simple, fast, and easy to explain.\n",
    "\n",
    "The Reality (Concave/Non-Convex): Real human behavior often forms weird shapes. Imagine a \"donut\" shape or a \"banana\" shape of data points.\n",
    "- E.g. DBSCAN - Density-Based Spatial Clustering of Applications with Noise\n",
    "- It does not assume clusters are spheres. It groups points that are packed closely together. If your shoppers form a \"banana\" shape, DBSCAN will find the banana. It also handles outliers (noise) well.\n",
    "\n",
    "If your data is shaped like a banana (concave), K-Means will fail. It tries to force a circle around the banana, often cutting it in half and putting the two halves in different clusters. This is mathematically \"correct\" for the algorithm, but logically wrong for your analysis.\n",
    "\n",
    "> Mention both methods as options that we will compare, use K-means as the baseline as it is the standard clustering method and acknowledge that shoppers might form irregular (concave) groups so we will check for that\n",
    "\n",
    "The Winning Narrative:\n",
    "- \"We applied K-Means (Baseline).\"\n",
    "- \"We observed that K-Means failed to capture the complex structure (evaluated via Silhouette score/Visuals).\"\n",
    "- \"Therefore, we advanced to DBSCAN, which successfully captured the non-convex 'concave' clusters.\"\n",
    "- \"This improved our prediction accuracy by X%.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624c1d2",
   "metadata": {},
   "source": [
    "### Alternative text about clustering\n",
    "\n",
    "\"We will primarily apply K-Means clustering to identify behavioral segments. To prepare the mixed dataset (numerical and categorical), we will apply One-Hot Encoding and standardize the skewed numerical features (e.g., duration metrics).\n",
    "\n",
    "Addressing Cluster Geometry: We acknowledge that K-Means assumes convex (spherical) clusters, which may not align with complex user behaviors (potentially concave or irregular distributions). Therefore, we will not rely solely on the Elbow Method. We will cross-validate cluster quality using the Silhouette Coefficient and visualize the data using PCA or t-SNE to inspect separation. If K-Means fails to identify distinct groups, we will explore density-based algorithms like DBSCAN, which can handle non-convex clusters and noise effectively.\"\n",
    "\n",
    "> NOTE: Do not know much about these so research needed before adding \"Silhouette Coefficient and visualize the data using PCA or t-SNE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e35482",
   "metadata": {},
   "source": [
    "### Other things that might need to be mentioned\n",
    "\n",
    "#### The \"Multicollinearity\" Trap (Double Counting)\n",
    "\n",
    "Your dataset contains features that are almost identical.\n",
    "\n",
    "The Culprits: BounceRates and ExitRates.\n",
    "\n",
    "Bounce Rate: You enter the site and leave immediately without triggering any other request.\n",
    "\n",
    "Exit Rate: You leave the site from this page (regardless of how many pages you visited before).\n",
    "\n",
    "The Problem: These two numbers are highly correlated (if Bounce goes up, Exit usually goes up).\n",
    "\n",
    "\n",
    "Why it breaks models: You list Logistic Regression  as a model. Logistic Regression struggles when two input variables are highly correlated (Multicollinearity). It makes the model unstable and the coefficients uninterpretable.\n",
    "\n",
    "> Add a line in 1. Data Preparation: \"We will check for multicollinearity (using a Correlation Matrix or VIF) and remove redundant features, specifically examining the high correlation between Bounce Rates and Exit Rates.\"\n",
    "\n",
    "> NOTE: in Lecture 6 (GitLab) we had about correlation matrices etc\n",
    "\n",
    "\n",
    "#### The \"Data Leakage\" Trap (The Cluster-Predict Paradox)\n",
    "\n",
    "This is the most common error in \"Cluster-then-Predict\" projects.\n",
    "\n",
    "The Scenario:\n",
    "- You take your entire dataset (12,000 rows).\n",
    "- You run Clustering on all of it.\n",
    "- You assign a cluster label (e.g., \"Impulsive Shopper\") to every row.\n",
    "- Then you split the data into Training (80%) and Test (20%) sets for your Supervised prediction.\n",
    "\n",
    "The Error: You just cheated! Your \"Test Set\" rows were used to calculate the cluster centers. Information from the test set has \"leaked\" into the training process. Your results will look artificially good, but the model will fail in real life.\n",
    "\n",
    "> \"To prevent data leakage, the Unsupervised Clustering will be fit only on the training set. The resulting cluster centroids will then be used to assign labels to the test set instances.\"\n",
    "\n",
    "NOTE: In the context of K-Means clustering, a **Centroid** is simply the geometric center of a cluster. Think of it as the \"average behavior\" of all the people in that specific group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bcefc",
   "metadata": {},
   "source": [
    "### DBSCAN does not have .predict()\n",
    "\n",
    "Can we use k-NN (k-Nearest Neighbors) with DBSCAN?\n",
    "There is a slight confusion here:\n",
    "- k-NN is Supervised: It needs labels (e.g., \"Buyer\" vs \"Non-Buyer\") to work. You use it to predict new data.\n",
    "- Clustering is Unsupervised: It creates the labels.\n",
    "\n",
    "If you want to keep the \"irregular shapes\" argument (DBSCAN) but make it work for your exam, you can use k-NN as a helper:\n",
    "\n",
    "1. Cluster the training data using DBSCAN (Unsupervised).\n",
    "2. Train a k-NN model using those new cluster labels.\n",
    "3. Predict the cluster for the test set using the k-NN model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "purchase-intent-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
